{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NLP Assignment-\n",
        "By-Prinsh kumar raj"
      ],
      "metadata": {
        "id": "0zzXQ9ZllMs7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TJrPapETJ2F",
        "outputId": "6a613463-a246-41e4-b1ea-09846f4fa422"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural', 'Language', 'Processing', 'with', 'Python', 'is', 'fun', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download the 'punkt_tab' data package\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "sentence = \"Natural Language Processing with Python is fun.\"\n",
        "\n",
        "tokens = word_tokenize(sentence)\n",
        "print(tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "sentence = \"Hello there! How's the weather today?\"\n",
        "\n",
        "tokens = word_tokenize(sentence)\n",
        "\n",
        "tokens_without_punctuation = [word for word in tokens if word not in string.punctuation]\n",
        "\n",
        "print(tokens_without_punctuation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEAiK7BWTT-i",
        "outputId": "1d359dba-f48b-4bc4-85ad-395aa4b785a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'there', 'How', \"'s\", 'the', 'weather', 'today']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "\n",
        "sentence = \"This is a simple sentence for stopword removal.\"\n",
        "\n",
        "tokens = word_tokenize(sentence)\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "tokens_without_stopwords = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "print(tokens_without_stopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usj2Zs4xULjc",
        "outputId": "98ee7db4-059a-4f78-efa9-6db00a0402d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['simple', 'sentence', 'stopword', 'removal', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "sentence = \"The striped bats are hanging on their feet for best.\"\n",
        "\n",
        "tokens = word_tokenize(sentence)\n",
        "\n",
        "stemmed_words = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "print(stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTh469sYUieC",
        "outputId": "c0f8e4ec-b1cd-45d9-dc60-79d36c0ff789"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'stripe', 'bat', 'are', 'hang', 'on', 'their', 'feet', 'for', 'best', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "sentence = \"The geese are flying south for the winter.\"\n",
        "\n",
        "tokens = word_tokenize(sentence)\n",
        "\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "print(lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCrZqSGtUy7F",
        "outputId": "138712c6-0f45-428f-83b4-7a7b3903ffb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'goose', 'are', 'flying', 'south', 'for', 'the', 'winter', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "sentence = \"Hello, World! NLP with Python.\"\n",
        "\n",
        "sentence = sentence.lower()\n",
        "\n",
        "sentence_without_punctuation = ''.join([char for char in sentence if char not in string.punctuation])\n",
        "\n",
        "print(sentence_without_punctuation)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIOLIBnPVUgj",
        "outputId": "8d8a68e7-32ba-4946-aa7b-46559dc5f085"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello world nlp with python\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "paragraph = \"Hello World. This is NLTK. Let's explore NLP!\"\n",
        "\n",
        "sentences = sent_tokenize(paragraph)\n",
        "\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wu8GflOeWI_f",
        "outputId": "547f569a-38d5-4055-887d-f1a81fe1976a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello World.', 'This is NLTK.', \"Let's explore NLP!\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "stemmer = LancasterStemmer()\n",
        "\n",
        "sentence = \"Loving the experience of learning NLTK\"\n",
        "\n",
        "tokens = word_tokenize(sentence)\n",
        "\n",
        "stemmed_words = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "print(stemmed_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PS0_CkItWx-R",
        "outputId": "c3f6541f-db39-4e47-8414-c782f88ca493"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['lov', 'the', 'expery', 'of', 'learn', 'nltk']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "\n",
        "sentence = \"This is a test sentence, with stopwords and punctuation!\"\n",
        "\n",
        "tokens = word_tokenize(sentence)\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words and word not in string.punctuation]\n",
        "\n",
        "print(filtered_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycDExZrVaWDf",
        "outputId": "f04fd76b-cfd8-4151-e816-3eab60f966df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['test', 'sentence', 'stopwords', 'punctuation']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "sentence = \"The striped bats are hanging on their feet.\"\n",
        "\n",
        "tokens = word_tokenize(sentence)\n",
        "\n",
        "pos_tags = pos_tag(tokens)\n",
        "\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return 'a'\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return 'v'\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return 'n'\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return 'r'\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "lemmatized_words = [\n",
        "    lemmatizer.lemmatize(word, get_wordnet_pos(pos) or 'n')\n",
        "    for word, pos in pos_tags\n",
        "]\n",
        "\n",
        "print(lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmxgQz20Cb5f",
        "outputId": "640603e6-39a9-4dc0-ec8c-bde504f6429f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'striped', 'bat', 'be', 'hang', 'on', 'their', 'foot', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import string\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "sentence = \"Running through the forest, the fox is faster.\"\n",
        "\n",
        "tokens = word_tokenize(sentence)\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "processed_tokens = [\n",
        "    stemmer.stem(word) for word in tokens\n",
        "    if word.lower() not in stop_words and word not in string.punctuation\n",
        "]\n",
        "\n",
        "print(processed_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsjtI107msCC",
        "outputId": "b849e009-b213-4248-89ce-a9d32ff09886"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['run', 'forest', 'fox', 'faster']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "sentence = \"This is an example sentence for counting stopwords.\"\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens = word_tokenize(sentence)\n",
        "stopword_count = sum(1 for word in tokens if word.lower() in stop_words)\n",
        "print(stopword_count)\n"
      ],
      "metadata": {
        "id": "BqflBfsLn88v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4033c2a9-94be-4bed-d05e-f9ca609ad332"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "sentence = \"Stemming, punctuation! Removal example.\"\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "stemmer = PorterStemmer()\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
        "print(stemmed_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YE6Rpw7xBZQC",
        "outputId": "8b54ffbd-4e21-41cd-9f9d-919d8a735561"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['stem', 'punctuat', 'remov', 'exampl']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "sentence = \"Punctuation removal with regex in NLP!\"\n",
        "sentence = re.sub(r'[^\\w\\s]', '', sentence)\n",
        "tokens = word_tokenize(sentence)\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klgcRPHRDFMl",
        "outputId": "fff2b0d6-99f9-4dab-8afc-ac3c37d7ca1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Punctuation', 'removal', 'with', 'regex', 'in', 'NLP']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download the 'stopwords' dataset if it's not already downloaded.\n",
        "nltk.download('stopwords')\n",
        "# Download the 'wordnet' dataset, which is required for lemmatization\n",
        "nltk.download('wordnet')\n",
        "\n",
        "sentence = \"The dogs are barking loudly.\"\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "tokens = word_tokenize(sentence)\n",
        "filtered_tokens = [lemmatizer.lemmatize(word) for word in tokens if word.lower() not in stop_words]\n",
        "print(filtered_tokens)"
      ],
      "metadata": {
        "id": "00nuVnA4DY1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71523bde-3586-4c54-e1c0-3509b19da948"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['dog', 'barking', 'loudly', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9VsNxr4YIhkS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}